{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pip install regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pandas_datareader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_datareader as wb\n",
    "from pandas_datareader import data\n",
    "import yfinance as yf\n",
    "import requests \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from lxml import html\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import norm\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from scipy import stats\n",
    "from regressors import stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_treasury_rate():\n",
    "    # web-scrape the data: Daily Treasury Yield Curve Rates\n",
    "    url = 'https://www.treasury.gov/resource-center/data-chart-center/interest-rates/Pages/TextView.aspx?data=yield'\n",
    "    r = requests.get(url)\n",
    "    html = r.text\n",
    "\n",
    "    soup = BeautifulSoup(html)\n",
    "    table = soup.find('table', {\"class\": \"t-chart\"})\n",
    "    rows = table.find_all('tr')\n",
    "    data = []\n",
    "    for row in rows[1:]:\n",
    "        cols = row.find_all('td')\n",
    "        cols = [ele.text.strip() for ele in cols]\n",
    "        data.append([ele for ele in cols if ele])\n",
    "\n",
    "        result = pd.DataFrame(data, columns=['Date', '1 Mo', '2 Mo', '3 Mo', '6 Mo', '1 Yr', '2 Yr', '3 Yr', '5 Yr', '7 Yr', '10 Yr', '20 Yr', '30 Yr'])\n",
    "\n",
    "    return(result)\n",
    "\n",
    "\n",
    "\n",
    "def return_df(close_price_df):\n",
    "        \n",
    "    return_df = close_price_df.pct_change().apply(lambda x: np.log(1+x))\n",
    "    \n",
    "    return return_df\n",
    "\n",
    "def dummies_beater(all_returns,Rf, option = \"return\"):\n",
    "    \n",
    "    all_returns = all_returns.dropna(how=\"all\")\n",
    "    Rm = all_returns[\"SPY\"]\n",
    "    Rm = Rm.fillna(0)\n",
    "    comps_R = all_returns.drop(columns = [\"SPY\"])\n",
    "    comps_R = comps_R.fillna(0)\n",
    "    \n",
    "    if option == \"return\":\n",
    "        comps_R_excess = comps_R.subtract(Rm.values, axis=0)\n",
    "        dummies_Return_beaters = comps_R_excess.copy()\n",
    "        dummies_Return_beaters[dummies_Return_beaters >= 0] =1\n",
    "        dummies_Return_beaters[dummies_Return_beaters < 0] =0\n",
    "        \n",
    "    elif option == \"sharpe\":\n",
    "        market_volatility = Rm.std()\n",
    "        comps_volatility = comps_R.std()\n",
    "        comps_sharpe = (comps_R - Rf)/comps_volatility\n",
    "        market_sharpe = (Rm - Rf)/market_volatility\n",
    "        sharpe_excess = comps_sharpe.subtract(market_sharpe, axis=0)\n",
    "        dummies_Return_beaters = sharpe_excess.copy()\n",
    "        dummies_Return_beaters[dummies_Return_beaters >= 0] =1\n",
    "        dummies_Return_beaters[dummies_Return_beaters < 0] =0\n",
    "        \n",
    "    return dummies_Return_beaters\n",
    "\n",
    "def port_simulation(stock_closed_df):\n",
    "    \n",
    "    pct_change_df = stock_closed_df.pct_change()\n",
    "    ind_er = pct_change_df.mean()\n",
    "    return_df = pct_change_df.apply(lambda x: np.log(1+x))\n",
    "    cov_matrix = return_df.cov()\n",
    "    \n",
    "    p_ret = [] \n",
    "    p_vol = [] \n",
    "    p_weights = [] \n",
    "    \n",
    "    \n",
    "    num_assets = len(stock_closed_df.columns)\n",
    "    num_portfolios = 10000\n",
    "\n",
    "    \n",
    "    for portfolio in range(num_portfolios):\n",
    "        weights = np.random.random(num_assets)\n",
    "        weights = weights/np.sum(weights)\n",
    "        p_weights.append(weights)\n",
    "        returns = np.dot(weights, ind_er)  \n",
    "        p_ret.append(returns)\n",
    "        var = cov_matrix.mul(weights, axis=0).mul(weights, axis=1).sum().sum()\n",
    "        sd = np.sqrt(var)  \n",
    "        p_vol.append(sd)\n",
    "    \n",
    "    data = {'Returns':p_ret, 'Volatility':p_vol}\n",
    " \n",
    "    for counter, symbol in enumerate(stock_closed_df.columns.tolist()):\n",
    "        data[symbol] = [w[counter] for w in p_weights]\n",
    "        portfolios  = pd.DataFrame(data)\n",
    "        \n",
    "    return portfolios\n",
    "\n",
    "def portfolios_return_df(equity_selection, all_return):\n",
    "    \n",
    "    simulation = port_simulation(equity_selection)\n",
    "    weight_possibility = simulation.drop(columns=[\"Returns\", \"Volatility\"])\n",
    "    portfolio_return_df = pd.DataFrame(columns = weight_possibility.index)\n",
    "    port_comps_return = all_return[weight_possibility.columns]\n",
    "    \n",
    "    for portfolio in tqdm(weight_possibility.index):\n",
    "        portfolio_return = weight_possibility.iloc[portfolio].mul(port_comps_return).T.sum()\n",
    "        portfolio_return_df[portfolio] = portfolio_return\n",
    "    \n",
    "    return portfolio_return_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FF_assign_label(index_comp_info):\n",
    "        \n",
    "        \n",
    "    index_comp_info[\"bookToMarket\"] = 1/index_comp_info[\"PB_ratio\"]\n",
    "    index_comp_info[\"Small_Big_Cap\"] = index_comp_info[\"mkt_cap\"].map(lambda x: \"B\" if x >= index_comp_info[\"mkt_cap\"].median() else \"S\")\n",
    "    \n",
    "    lower, upper = index_comp_info[\"bookToMarket\"].quantile([0.3, 0.7])\n",
    "    index_comp_info[\"HML_BP\"] = index_comp_info[\"bookToMarket\"].map(lambda x: \"H\" if x >= upper else \"M\")\n",
    "    index_comp_info[\"HML_BP\"] = index_comp_info.apply(lambda row: \"L\" if row[\"bookToMarket\"] <= lower else row[\"HML_BP\"], axis = 1)\n",
    "    \n",
    "    lower_roe, upper_roe = index_comp_info[\"ROE\"].quantile([0.3, 0.7])\n",
    "    index_comp_info[\"RNW_ROE\"] = index_comp_info[\"ROE\"].map(lambda x: \"R\" if x >= upper_roe else \"N\")\n",
    "    index_comp_info[\"RNW_ROE\"] = index_comp_info.apply(lambda row: \"W\" if row[\"ROE\"] <= lower_roe else row[\"RNW_ROE\"], axis = 1)\n",
    "    \n",
    "    lower_invest, upper_invest = index_comp_info[\"Asset_growth\"].quantile([0.3, 0.7])\n",
    "    index_comp_info[\"ANC_investment\"] = index_comp_info[\"Asset_growth\"].map(lambda x: \"A\" if x >= upper_invest else \"N\")\n",
    "    index_comp_info[\"ANC_investment\"] = index_comp_info.apply(lambda row: \"C\" if row[\"Asset_growth\"] <= lower_invest else row[\"ANC_investment\"], axis = 1)\n",
    "            \n",
    "    return index_comp_info\n",
    "        \n",
    "def FF_factor_classifier(index_comp_info_with_label):\n",
    "        \n",
    "    data = index_comp_info_with_label\n",
    "    Small_Low = data.query('(Small_Big_Cap==\"S\") & (HML_BP==\"L\")')\n",
    "    Small_Mid = data.query('(Small_Big_Cap==\"S\") & (HML_BP==\"M\")')\n",
    "    Small_High = data.query('(Small_Big_Cap==\"S\") & (HML_BP==\"H\")')\n",
    "    \n",
    "    Small_Weak = data.query('(Small_Big_Cap==\"S\") & (RNW_ROE==\"W\")')\n",
    "    Small_Neutral_Profit = data.query('(Small_Big_Cap==\"S\") & (RNW_ROE==\"N\")')\n",
    "    Small_Robust = data.query('(Small_Big_Cap==\"S\") & (RNW_ROE==\"R\")')\n",
    "    \n",
    "    Small_Conservative =  data.query('(Small_Big_Cap==\"S\") & (ANC_investment==\"C\")')\n",
    "    Small_Neutral_Invest =  data.query('(Small_Big_Cap==\"S\") & (ANC_investment==\"N\")')\n",
    "    Small_Aggresive =  data.query('(Small_Big_Cap==\"S\") & (ANC_investment==\"A\")')\n",
    "    \n",
    "    Big_Low = data.query('(Small_Big_Cap==\"B\") & (HML_BP==\"L\")')\n",
    "    Big_Mid = data.query('(Small_Big_Cap==\"B\") & (HML_BP==\"M\")')\n",
    "    Big_High = data.query('(Small_Big_Cap==\"B\") & (HML_BP==\"H\")')\n",
    "    \n",
    "    Big_Weak = data.query('(Small_Big_Cap==\"B\") & (RNW_ROE==\"W\")')\n",
    "    Big_Neutral_Profit = data.query('(Small_Big_Cap==\"B\") & (RNW_ROE==\"N\")')\n",
    "    Big_Robust = data.query('(Small_Big_Cap==\"B\") & (RNW_ROE==\"R\")')\n",
    "\n",
    "    Big_Conservative =  data.query('(Small_Big_Cap==\"B\") & (ANC_investment==\"C\")')\n",
    "    Big_Neutral_Invest = data.query('(Small_Big_Cap==\"B\") & (ANC_investment==\"N\")')\n",
    "    Big_Aggresive =  data.query('(Small_Big_Cap==\"B\") & (ANC_investment==\"A\")')\n",
    "    \n",
    "    each_groups_list = [Small_Low, Small_Mid, Small_High, \n",
    "                            Small_Weak, Small_Neutral_Profit, Small_Robust,\n",
    "                            Small_Conservative, Small_Neutral_Invest, Small_Aggresive,\n",
    "                            Big_Low, Big_Mid,Big_High,\n",
    "                            Big_Weak, Big_Neutral_Profit, Big_Robust,\n",
    "                            Big_Conservative, Big_Neutral_Invest, Big_Aggresive]\n",
    "        \n",
    "    return each_groups_list\n",
    "    \n",
    "def FF_classes_return(market_components_return, list_of_group_info, axis=True):\n",
    "        \n",
    "    groups_names = [\"Small_Low\", \"Small_Mid\", \"Small_High\",\n",
    "                        \"Small_Weak\", \"Small_Neutral_Profit\", \"Small_Robust\",\n",
    "                        \"Small_Cons\", \"Small_Neutral_Invest\", \"Small_Aggr\",\n",
    "                        \"Big_Low\", \"Big_Mid\",\"Big_High\",\n",
    "                        \"Big_Weak\", \"Big_Neutral_Profit\", \"Big_Robust\",\n",
    "                        \"Big_Cons\", \"Big_Neutral_Invest\", \"Big_Aggr\"]\n",
    "    \n",
    "    df_groups = pd.DataFrame(columns = groups_names)\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    for group in list_of_group_info:\n",
    "    \n",
    "        group_cap = group[\"mkt_cap\"].T\n",
    "        group_total_cap = group[\"mkt_cap\"].sum()\n",
    "        group_cap_multi_return = group_cap*market_components_return[list(group.index)]\n",
    "        \n",
    "        if axis == True:\n",
    "            df_groups[groups_names[counter]] = group_cap_multi_return.apply(lambda row: row.sum()/group_total_cap, axis=1)\n",
    "        \n",
    "        else:\n",
    "            groups_index_return = group_cap_multi_return.sum()/group_total_cap\n",
    "            df_groups[groups_names[counter]] = [groups_index_return]\n",
    "    \n",
    "        counter += 1\n",
    "                \n",
    "    return df_groups\n",
    "    \n",
    "def FF_calc_factors(classes_return_df, df = True):\n",
    "    \n",
    "    factor_name = [\"SMB\", \"HML\", \"RMW\", \"CMA\"]\n",
    "    \n",
    "    SMB_BP = (classes_return_df[\"Small_Low\"] + classes_return_df[\"Small_Mid\"] \n",
    "                      + classes_return_df[\"Small_High\"]) - (classes_return_df[\"Big_Low\"]\n",
    "                      + classes_return_df[\"Big_Mid\"] + classes_return_df[\"Big_High\"])/3\n",
    "    \n",
    "    SMB_PFT = (classes_return_df[\"Small_Weak\"] + classes_return_df[\"Small_Neutral_Profit\"] \n",
    "                      + classes_return_df[\"Small_Robust\"]) - (classes_return_df[\"Big_Weak\"]\n",
    "                      + classes_return_df[\"Big_Neutral_Profit\"] + classes_return_df[\"Big_Robust\"])/3\n",
    "    \n",
    "    SMB_INV = (classes_return_df[\"Small_Cons\"] + classes_return_df[\"Small_Neutral_Invest\"] \n",
    "                      + classes_return_df[\"Small_Aggr\"]) - (classes_return_df[\"Big_Cons\"]\n",
    "                      + classes_return_df[\"Big_Neutral_Invest\"] + classes_return_df[\"Big_Aggr\"])/3\n",
    "    \n",
    "    if df == True:\n",
    "        \n",
    "        FF_factors_data = pd.DataFrame(columns = factor_name)\n",
    "        \n",
    "    \n",
    "    FF_factors_data[\"SMB\"] = (SMB_BP + SMB_PFT + SMB_INV)/3\n",
    "    \n",
    "    FF_factors_data[\"HML\"] = (classes_return_df[\"Small_High\"] + classes_return_df[\"Big_High\"]\n",
    "                      - (classes_return_df[\"Small_Low\"] + classes_return_df[\"Big_Low\"])) / 2\n",
    "    \n",
    "    FF_factors_data[\"RMW\"] = (classes_return_df[\"Small_Robust\"] + classes_return_df[\"Big_Robust\"]\n",
    "                      - (classes_return_df[\"Small_Weak\"] + classes_return_df[\"Big_Weak\"])) / 2\n",
    "    \n",
    "    FF_factors_data[\"CMA\"] = (classes_return_df[\"Small_Cons\"] + classes_return_df[\"Big_Cons\"]\n",
    "                      - (classes_return_df[\"Small_Aggr\"] + classes_return_df[\"Big_Aggr\"])) / 2\n",
    "        \n",
    "    return FF_factors_data\n",
    "\n",
    "    \n",
    "def FF_regress(FF_factors_df, target_comp_risk_premium):\n",
    "            \n",
    "    y = target_comp_risk_premium\n",
    "    X = FF_factors_df\n",
    "    model = linear_model.LinearRegression()\n",
    "    model.fit(X.astype(float), y.astype(float))\n",
    "    params = np.append(model.intercept_,model.coef_)\n",
    "    predictions = model.predict(X)\n",
    "\n",
    "    newX = pd.DataFrame({\"Constant\":np.ones(len(X))}).join(pd.DataFrame(X.reset_index(drop=True)))\n",
    "    MSE = (sum((y-predictions)**2))/(len(newX)-len(newX.columns))\n",
    "\n",
    "    # Note if you don't want to use a DataFrame replace the two lines above with\n",
    "    # newX = np.append(np.ones((len(X),1)), X, axis=1)\n",
    "    # MSE = (sum((y-predictions)**2))/(len(newX)-len(newX[0]))\n",
    "\n",
    "    var_b = MSE*(np.linalg.inv(np.dot(newX.T,newX)).diagonal())\n",
    "    sd_b = np.sqrt(var_b)\n",
    "    ts_b = params/ sd_b\n",
    "    #print(newX)\n",
    "    p_values =[2*(1-stats.t.cdf(np.abs(i),(len(newX)-len(X.columns)))) for i in ts_b]\n",
    "\n",
    "    sd_b = np.round(sd_b,3)\n",
    "    ts_b = np.round(ts_b,3)\n",
    "    p_values = np.round(p_values,3)\n",
    "    params = np.round(params,4)\n",
    "\n",
    "    myDF3 = pd.DataFrame()\n",
    "    myDF3[\"Coefficients\"],myDF3[\"Standard_Errors\"],myDF3[\"t_stat\"],myDF3[\"P_value\"] = [params,sd_b,ts_b,p_values]\n",
    "    \n",
    "    return myDF3\n",
    "    \n",
    "\n",
    "def regressAll(FF_factors, all_returns, days, train_base=2000, Rf=0.002):\n",
    "    \n",
    "    \n",
    "    dummies_Return_beaters = dummies_beater(all_returns, Rf)\n",
    "    \n",
    "    prob_10obs = dummies_Return_beaters.iloc[days - 10 : days].mean().T\n",
    "    prob_20obs = dummies_Return_beaters.iloc[days - 20 : days].mean().T\n",
    "    prob_30obs = dummies_Return_beaters.iloc[days - 30 : days].mean().T\n",
    "\n",
    "    target = dummies_Return_beaters.iloc[days+1]\n",
    "    \n",
    "    features_df = pd.DataFrame(columns = [\"const\",'B1', 'B2', 'B3', 'B4', 'B5',\"Pval_C\", \"Pval1\", \"Pval2\", \"Pval3\", \"Pval4\", \"Pval5\"],\n",
    "                               index = all_returns.columns)\n",
    "    \n",
    "    comps_R = all_returns.drop(columns = [\"SPY\"]).fillna(0)\n",
    "    FF_factors = FF_factors.fillna(0)\n",
    "    for i in comps_R:\n",
    "        \n",
    "        regression_stat = FF_regress(FF_factors.iloc[days-train_base:days], comps_R[i].iloc[days-train_base:days])\n",
    "        betas = list(regression_stat[\"Coefficients\"])\n",
    "        Pvals = list(regression_stat[\"P_value\"])\n",
    "        features = betas + Pvals\n",
    "        features_df.loc[i] = features\n",
    "    \n",
    "    features_df[\"prob_10obs\"], features_df[\"prob_20obs\"], features_df[\"prob_30obs\"] = [prob_10obs, prob_20obs, prob_30obs]\n",
    "    features_df[\"true_target\"] = target\n",
    "    \n",
    "    \n",
    "    return features_df\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency: 30 min and 2 min\n",
    "\n",
    "Time series Beta estimation: Measure 5 betas for each 30 min, using 2 min frequency. (Need to know how betas change every 30 min)\n",
    "\n",
    "Plan B:\n",
    "1) Use historical return data (2m or higher frequency) of 505 companies in SP500 index to run FF for each company.\n",
    "\n",
    "2) Calculate probability of beating market in certain period (if APPL 2m Sharpe beated the market 5 times in last 30 min, prob of beating is 5/15 = 0.333).\n",
    "\n",
    "3) put 1) and 2) together to get a all companies' prob of winning in given period and their Betas.\n",
    "\n",
    "4) Model probability from 3)\n",
    "\n",
    "5) Caculate Betas for all 10,000 simulated potfolio\n",
    "\n",
    "6) get the probability of winning\n",
    "\n",
    "How to make it predictive?\n",
    "\n",
    "Model Construction: This may not work\n",
    "\n",
    "1) model the probability of 30 min winning with 5 betas(5 features) we eatimate every 30 min. (can use SVM, decision tree, regression...)\n",
    "\n",
    "2) measure betas of winner-loser portfolios spread for same time length for our portolios\n",
    "\n",
    "3) plug betas of portfolios to the model in 1)\n",
    "\n",
    "4) model true prob of winning of portfolios on fitted value from 3)\n",
    "\n",
    "5) make analyical model to preidictive model\n",
    "\n",
    "The reason we estimate the probability of picking a winner portfolio with given beta is because as we move from Time(T) T1 to T2, the stock price changes, so our weight also changes. We are essensially move from one simulated portfolio to another simulated portfolio as the time goes.\n",
    "\n",
    "\n",
    "What ML can apply here?\n",
    "\n",
    "What would the result be if use unsupervised grouping to caculate factors? (Maybe useful when FF doesn't explain well in term of R^2)\n",
    "\n",
    "Any potential non-linear relationship?\n",
    "\n",
    "How to optimize time scale to ensure the highest predictive ability? or would models' ability consistant accross time series mapping?\n",
    "\n",
    "How to test predictivity?\n",
    "\n",
    "Any trade off between high winning prob and portfolio sharpe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bedu/CSS100_Project\n",
      "['CSS100.ipynb', '.ipynb_checkpoints', 'SPYn500_30m_close.csv', 'Fama_French_info.csv', 'winner_loser_spread_data.csv', '.git']\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "print(os.listdir(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir(\"/Users/liuhengjia/Desktop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "FF_info = pd.read_csv(\"Fama_French_info.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 = ' '\n",
    "index_list = FF_info.index.tolist()\n",
    "index_list.append(\"SPY\")\n",
    "#index_list.reverse()\n",
    "total_string = str1.join(index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPYn500_30m_df = yf.download(total_string, start = '2021-10-11', end = \"2021-11-18\", interval = '30m')\n",
    "#SPYn500_2m_df = yf.download(total_string, start = '2021-10-11', end = \"2021-11-18\", interval = '2m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPYn500_30m_df[\"Adj Close\"].to_csv(\"SPYn500_30m_close.csv\")\n",
    "#SPYn500_2m_df[\"Adj Close\"].to_csv(\"SPYn500_2m_close.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_2m_close = pd.read_csv(\"SPYn500_30m_close.csv\", index_col = 0)\n",
    "#sp_30m_close = pd.read_csv(\"data/SPYn500_30m_close.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>AAL</th>\n",
       "      <th>AAP</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>ABBV</th>\n",
       "      <th>ABC</th>\n",
       "      <th>ABMD</th>\n",
       "      <th>ABT</th>\n",
       "      <th>ACN</th>\n",
       "      <th>ADBE</th>\n",
       "      <th>...</th>\n",
       "      <th>XEL</th>\n",
       "      <th>XLNX</th>\n",
       "      <th>XOM</th>\n",
       "      <th>XRAY</th>\n",
       "      <th>XYL</th>\n",
       "      <th>YUM</th>\n",
       "      <th>ZBH</th>\n",
       "      <th>ZBRA</th>\n",
       "      <th>ZION</th>\n",
       "      <th>ZTS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-10-11 09:30:00-04:00</th>\n",
       "      <td>151.880005</td>\n",
       "      <td>20.120001</td>\n",
       "      <td>214.960007</td>\n",
       "      <td>143.929993</td>\n",
       "      <td>111.239998</td>\n",
       "      <td>122.559998</td>\n",
       "      <td>335.290009</td>\n",
       "      <td>118.489998</td>\n",
       "      <td>327.940002</td>\n",
       "      <td>578.719971</td>\n",
       "      <td>...</td>\n",
       "      <td>62.715000</td>\n",
       "      <td>157.759995</td>\n",
       "      <td>62.610001</td>\n",
       "      <td>58.150002</td>\n",
       "      <td>120.910004</td>\n",
       "      <td>123.790001</td>\n",
       "      <td>146.869995</td>\n",
       "      <td>499.000000</td>\n",
       "      <td>64.120003</td>\n",
       "      <td>198.039993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-11 10:00:00-04:00</th>\n",
       "      <td>151.809998</td>\n",
       "      <td>20.155001</td>\n",
       "      <td>215.559998</td>\n",
       "      <td>144.701096</td>\n",
       "      <td>111.160004</td>\n",
       "      <td>123.070000</td>\n",
       "      <td>337.554993</td>\n",
       "      <td>118.800003</td>\n",
       "      <td>328.140015</td>\n",
       "      <td>582.020020</td>\n",
       "      <td>...</td>\n",
       "      <td>62.930000</td>\n",
       "      <td>158.820007</td>\n",
       "      <td>62.669998</td>\n",
       "      <td>57.970001</td>\n",
       "      <td>120.809998</td>\n",
       "      <td>123.660004</td>\n",
       "      <td>147.750000</td>\n",
       "      <td>500.059998</td>\n",
       "      <td>64.269997</td>\n",
       "      <td>198.820007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-11 10:30:00-04:00</th>\n",
       "      <td>152.039993</td>\n",
       "      <td>20.245001</td>\n",
       "      <td>214.880005</td>\n",
       "      <td>144.445007</td>\n",
       "      <td>111.135002</td>\n",
       "      <td>123.080002</td>\n",
       "      <td>337.950012</td>\n",
       "      <td>118.666397</td>\n",
       "      <td>328.119995</td>\n",
       "      <td>580.059998</td>\n",
       "      <td>...</td>\n",
       "      <td>62.790001</td>\n",
       "      <td>158.580002</td>\n",
       "      <td>62.595001</td>\n",
       "      <td>58.029999</td>\n",
       "      <td>120.485001</td>\n",
       "      <td>123.519997</td>\n",
       "      <td>146.750000</td>\n",
       "      <td>500.789612</td>\n",
       "      <td>63.959999</td>\n",
       "      <td>198.339996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 506 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    A        AAL         AAP        AAPL  \\\n",
       "Datetime                                                                   \n",
       "2021-10-11 09:30:00-04:00  151.880005  20.120001  214.960007  143.929993   \n",
       "2021-10-11 10:00:00-04:00  151.809998  20.155001  215.559998  144.701096   \n",
       "2021-10-11 10:30:00-04:00  152.039993  20.245001  214.880005  144.445007   \n",
       "\n",
       "                                 ABBV         ABC        ABMD         ABT  \\\n",
       "Datetime                                                                    \n",
       "2021-10-11 09:30:00-04:00  111.239998  122.559998  335.290009  118.489998   \n",
       "2021-10-11 10:00:00-04:00  111.160004  123.070000  337.554993  118.800003   \n",
       "2021-10-11 10:30:00-04:00  111.135002  123.080002  337.950012  118.666397   \n",
       "\n",
       "                                  ACN        ADBE  ...        XEL        XLNX  \\\n",
       "Datetime                                           ...                          \n",
       "2021-10-11 09:30:00-04:00  327.940002  578.719971  ...  62.715000  157.759995   \n",
       "2021-10-11 10:00:00-04:00  328.140015  582.020020  ...  62.930000  158.820007   \n",
       "2021-10-11 10:30:00-04:00  328.119995  580.059998  ...  62.790001  158.580002   \n",
       "\n",
       "                                 XOM       XRAY         XYL         YUM  \\\n",
       "Datetime                                                                  \n",
       "2021-10-11 09:30:00-04:00  62.610001  58.150002  120.910004  123.790001   \n",
       "2021-10-11 10:00:00-04:00  62.669998  57.970001  120.809998  123.660004   \n",
       "2021-10-11 10:30:00-04:00  62.595001  58.029999  120.485001  123.519997   \n",
       "\n",
       "                                  ZBH        ZBRA       ZION         ZTS  \n",
       "Datetime                                                                  \n",
       "2021-10-11 09:30:00-04:00  146.869995  499.000000  64.120003  198.039993  \n",
       "2021-10-11 10:00:00-04:00  147.750000  500.059998  64.269997  198.820007  \n",
       "2021-10-11 10:30:00-04:00  146.750000  500.789612  63.959999  198.339996  \n",
       "\n",
       "[3 rows x 506 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_2m_close.iloc[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate log-return (assuming return is log-normal) for both time interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_2m_return = return_df(sp_2m_close)\n",
    "#total_30m_return = return_df(sp_30m_close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_2m_return = total_2m_return.dropna(how = \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>AAL</th>\n",
       "      <th>AAP</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>ABBV</th>\n",
       "      <th>ABC</th>\n",
       "      <th>ABMD</th>\n",
       "      <th>ABT</th>\n",
       "      <th>ACN</th>\n",
       "      <th>ADBE</th>\n",
       "      <th>...</th>\n",
       "      <th>XEL</th>\n",
       "      <th>XLNX</th>\n",
       "      <th>XOM</th>\n",
       "      <th>XRAY</th>\n",
       "      <th>XYL</th>\n",
       "      <th>YUM</th>\n",
       "      <th>ZBH</th>\n",
       "      <th>ZBRA</th>\n",
       "      <th>ZION</th>\n",
       "      <th>ZTS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-10-11 10:00:00-04:00</th>\n",
       "      <td>-0.000461</td>\n",
       "      <td>0.001738</td>\n",
       "      <td>0.002787</td>\n",
       "      <td>0.005343</td>\n",
       "      <td>-0.000719</td>\n",
       "      <td>0.004153</td>\n",
       "      <td>0.006733</td>\n",
       "      <td>0.002613</td>\n",
       "      <td>0.000610</td>\n",
       "      <td>0.005686</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003422</td>\n",
       "      <td>0.006697</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>-0.003100</td>\n",
       "      <td>-0.000827</td>\n",
       "      <td>-0.001051</td>\n",
       "      <td>0.005974</td>\n",
       "      <td>0.002122</td>\n",
       "      <td>0.002337</td>\n",
       "      <td>0.003931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-11 10:30:00-04:00</th>\n",
       "      <td>0.001514</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.003160</td>\n",
       "      <td>-0.001771</td>\n",
       "      <td>-0.000225</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.001170</td>\n",
       "      <td>-0.001125</td>\n",
       "      <td>-0.000061</td>\n",
       "      <td>-0.003373</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002227</td>\n",
       "      <td>-0.001512</td>\n",
       "      <td>-0.001197</td>\n",
       "      <td>0.001034</td>\n",
       "      <td>-0.002694</td>\n",
       "      <td>-0.001133</td>\n",
       "      <td>-0.006791</td>\n",
       "      <td>0.001458</td>\n",
       "      <td>-0.004835</td>\n",
       "      <td>-0.002417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-11 11:00:00-04:00</th>\n",
       "      <td>-0.001712</td>\n",
       "      <td>0.001589</td>\n",
       "      <td>-0.005647</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>-0.000495</td>\n",
       "      <td>0.001908</td>\n",
       "      <td>-0.002637</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>-0.000762</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004469</td>\n",
       "      <td>-0.000568</td>\n",
       "      <td>0.002473</td>\n",
       "      <td>-0.003452</td>\n",
       "      <td>-0.000706</td>\n",
       "      <td>-0.001539</td>\n",
       "      <td>-0.005329</td>\n",
       "      <td>0.000899</td>\n",
       "      <td>0.002030</td>\n",
       "      <td>-0.001312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 506 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  A       AAL       AAP      AAPL      ABBV  \\\n",
       "Datetime                                                                      \n",
       "2021-10-11 10:00:00-04:00 -0.000461  0.001738  0.002787  0.005343 -0.000719   \n",
       "2021-10-11 10:30:00-04:00  0.001514  0.004455 -0.003160 -0.001771 -0.000225   \n",
       "2021-10-11 11:00:00-04:00 -0.001712  0.001589 -0.005647  0.000035 -0.000495   \n",
       "\n",
       "                                ABC      ABMD       ABT       ACN      ADBE  \\\n",
       "Datetime                                                                      \n",
       "2021-10-11 10:00:00-04:00  0.004153  0.006733  0.002613  0.000610  0.005686   \n",
       "2021-10-11 10:30:00-04:00  0.000081  0.001170 -0.001125 -0.000061 -0.003373   \n",
       "2021-10-11 11:00:00-04:00  0.001908 -0.002637  0.000115 -0.000762 -0.000017   \n",
       "\n",
       "                           ...       XEL      XLNX       XOM      XRAY  \\\n",
       "Datetime                   ...                                           \n",
       "2021-10-11 10:00:00-04:00  ...  0.003422  0.006697  0.000958 -0.003100   \n",
       "2021-10-11 10:30:00-04:00  ... -0.002227 -0.001512 -0.001197  0.001034   \n",
       "2021-10-11 11:00:00-04:00  ... -0.004469 -0.000568  0.002473 -0.003452   \n",
       "\n",
       "                                XYL       YUM       ZBH      ZBRA      ZION  \\\n",
       "Datetime                                                                      \n",
       "2021-10-11 10:00:00-04:00 -0.000827 -0.001051  0.005974  0.002122  0.002337   \n",
       "2021-10-11 10:30:00-04:00 -0.002694 -0.001133 -0.006791  0.001458 -0.004835   \n",
       "2021-10-11 11:00:00-04:00 -0.000706 -0.001539 -0.005329  0.000899  0.002030   \n",
       "\n",
       "                                ZTS  \n",
       "Datetime                             \n",
       "2021-10-11 10:00:00-04:00  0.003931  \n",
       "2021-10-11 10:30:00-04:00 -0.002417  \n",
       "2021-10-11 11:00:00-04:00 -0.001312  \n",
       "\n",
       "[3 rows x 506 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_2m_return.iloc[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining components return, market return, dummies beater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_2m_return = total_2m_return.drop(columns=[\"SPY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rm_30m = total_30m_return[['SPY']]\n",
    "Rm_2m = total_2m_return[['SPY']]\n",
    "Rm_2m = Rm_2m.dropna(how=\"all\")\n",
    "#We need to discount Monthly Rf to 2m and 30m Rf using yield curve(continuously compounded)\n",
    "Rf = 0.000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies_2m_beater = dummies_beater(total_2m_return, Rf, option=\"sharpe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate FF factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "FF_info_labeled = FF_assign_label(FF_info)\n",
    "FF_classified_list = FF_factor_classifier(FF_info_labeled)\n",
    "FF_2m_classes_return = FF_classes_return(sp500_2m_return,FF_classified_list)\n",
    "FF_2m_factor = FF_calc_factors(FF_2m_classes_return)\n",
    "FF_2m_factor[\"Rm-Rf\"] = Rm_2m - Rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#equity_selection = sp_2m_close.drop(columns=[\"SPY\"]).sample(n=10, axis=\"columns\")\n",
    "#portfolio_2m_return = portfolios_return_df(equity_selection, total_2m_return)\n",
    "#portfolio_2m_return.to_csv(\"portfolio_2m_return.cvs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/portfolio_2m_return.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_112/86867553.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mportfolio_2m_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/portfolio_2m_return.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/portfolio_2m_return.csv'"
     ]
    }
   ],
   "source": [
    "portfolio_2m_return = pd.read_csv(\"data/portfolio_2m_return.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_2m_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'portfolio_2m_return' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_112/1103527290.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mportfolio_2m_return_Rm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mportfolio_2m_return\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mportfolio_2m_return_Rm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"SPY\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_2m_return\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"SPY\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'portfolio_2m_return' is not defined"
     ]
    }
   ],
   "source": [
    "portfolio_2m_return_Rm = portfolio_2m_return.copy()\n",
    "portfolio_2m_return_Rm[\"SPY\"] = total_2m_return[\"SPY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performence_df(FF_2m_factor,total_2m_return, portfolio_2m_return_Rm):\n",
    "\n",
    "    performence = pd.DataFrame(columns=[\"precision\", \"recall\", \"f1\"])\n",
    "\n",
    "    days = 2000\n",
    "\n",
    "    while days < len(portfolio_2m_return_Rm)-1:\n",
    "    \n",
    "        portfolio_feature = regressAll(FF_2m_factor,portfolio_2m_return_Rm, days ,2000)\n",
    "        \n",
    "        portfolio_feature = portfolio_feature.drop(index=[\"SPY\"]).fillna(0)\n",
    "        \n",
    "        XYtrain = regressAll(FF_2m_factor,total_2m_return,days,2000)\n",
    "        XYtrain = XYtrain.fillna(0)\n",
    "        \n",
    "        \n",
    "        sgd_clf = SGDClassifier(random_state = 42)\n",
    "        model = sgd_clf.fit(XYtrain.drop(columns=[\"true_target\"]),XYtrain[\"true_target\"])\n",
    "    \n",
    "        prediction = model.predict(portfolio_feature.drop(columns=[\"true_target\"]))\n",
    "        portfolio_feature[\"prediction\"] = prediction\n",
    "    \n",
    "        precision = precision_score(portfolio_feature[\"true_target\"], portfolio_feature[\"prediction\"])\n",
    "        recall = recall_score(portfolio_feature[\"true_target\"], portfolio_feature[\"prediction\"])\n",
    "        f1 = f1_score(portfolio_feature[\"true_target\"], portfolio_feature[\"prediction\"])\n",
    "    \n",
    "        score_series = pd.Series([precision, recall, f1], index = performence.columns)\n",
    "        performence = performence.append(score_series, ignore_index=True)\n",
    "    \n",
    "        days += 1\n",
    "    \n",
    "    return performence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'performence_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_112/2465581539.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperformence_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFF_2m_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtotal_2m_return\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mportfolio_2m_return_Rm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'performence_df' is not defined"
     ]
    }
   ],
   "source": [
    "p1 = performence_df(FF_2m_factor,total_2m_return, portfolio_2m_return_Rm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf = SGDClassifier(random_state = 42)\n",
    "model = sgd_clf.fit(XYtrain.drop(columns=[\"true_target\"]),XYtrain[\"true_target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_2m_return = total_2m_return.dropna(how = \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine betas measured above with 15 min prob of beating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regress prob of beating on observed betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_length = range(len(dummies_2m_beaters.index[:1000]))\n",
    "winner_rate_list = []\n",
    "for length in tqdm(time_length):\n",
    "    winner_rate = dummies_2m_beaters[\"GE\"].iloc[:length].mean()\n",
    "    winner_rate_list.append(winner_rate)\n",
    "    \n",
    "plt.scatter(time_length, winner_rate_list, s=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FF_regress(FF_2m_factor.fillna(0),sp500_2m_return[\"AAPL\"].fillna(0)-Rf )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressAll(FF_2m_factor.fillna(0), total_2m_return.fillna(0)-Rf, days=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification and Ensemble Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_classification(features, target_variables):\n",
    "    # ensemble methods: all binary classification\n",
    "    #ensemble modeling\n",
    "    X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "        # 3 binary classifications\n",
    "    log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
    "    rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    svm_clf = SVC(gamma=\"scale\", random_state=42)\n",
    "\n",
    "    voting_clf = VotingClassifier(\n",
    "        estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "        voting='soft')\n",
    "        # model averaging\n",
    "        # soft voting\n",
    "    return (log_clf + rnd_clf + svm_clf)/3\n",
    "        # if 1 is more, choose one;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'LogisticRegression' and 'RandomForestClassifier'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_112/1637392200.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbinary_classification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_2m_return\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_112/4044824948.py\u001b[0m in \u001b[0;36mbinary_classification\u001b[0;34m(features, target_variables)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# model averaging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# soft voting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlog_clf\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrnd_clf\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msvm_clf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;31m# if 1 is more, choose one;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'LogisticRegression' and 'RandomForestClassifier'"
     ]
    }
   ],
   "source": [
    "binary_classification(total_2m_return,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
