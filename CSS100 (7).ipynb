{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pip install regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_datareader as wb\n",
    "from pandas_datareader import data\n",
    "import yfinance as yf\n",
    "import requests \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from lxml import html\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import norm\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, r2_score\n",
    "from scipy import stats\n",
    "#from regressors import stats as st\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_treasury_rate():\n",
    "    \n",
    "    url = 'https://www.treasury.gov/resource-center/data-chart-center/interest-rates/Pages/TextView.aspx?data=yield'\n",
    "    r = requests.get(url)\n",
    "    html = r.text\n",
    "\n",
    "    soup = BeautifulSoup(html)\n",
    "    table = soup.find('table', {\"class\": \"t-chart\"})\n",
    "    rows = table.find_all('tr')\n",
    "    data = []\n",
    "    for row in rows[1:]:\n",
    "        cols = row.find_all('td')\n",
    "        cols = [ele.text.strip() for ele in cols]\n",
    "        data.append([ele for ele in cols if ele])\n",
    "\n",
    "        result = pd.DataFrame(data, columns=['Date', '1 Mo', '2 Mo', '3 Mo', '6 Mo', '1 Yr', '2 Yr', '3 Yr', '5 Yr', '7 Yr', '10 Yr', '20 Yr', '30 Yr'])\n",
    "\n",
    "    return(result)\n",
    "\n",
    "\n",
    "\n",
    "def return_df(close_price_df):\n",
    "        \n",
    "    return_df = close_price_df.pct_change().apply(lambda x: np.log(1+x))\n",
    "    \n",
    "    return return_df\n",
    "\n",
    "def dummies_beater(all_returns,Rf, option = \"return\"):\n",
    "    \n",
    "    all_returns = all_returns.dropna(how=\"all\")\n",
    "    Rm = all_returns[\"SPY\"]\n",
    "    Rm = Rm.fillna(0)\n",
    "    comps_R = all_returns.drop(columns = [\"SPY\"])\n",
    "    comps_R = comps_R.fillna(0)\n",
    "    \n",
    "    if option == \"return\":\n",
    "        comps_R_excess = comps_R.subtract(Rm.values, axis=0)\n",
    "        dummies_Return_beaters = comps_R_excess.copy()\n",
    "        dummies_Return_beaters[dummies_Return_beaters >= 0] =1\n",
    "        dummies_Return_beaters[dummies_Return_beaters < 0] =0\n",
    "        \n",
    "    elif option == \"sharpe\":\n",
    "        market_volatility = Rm.std()\n",
    "        comps_volatility = comps_R.std()\n",
    "        comps_sharpe = (comps_R - Rf)/comps_volatility\n",
    "        market_sharpe = (Rm - Rf)/market_volatility\n",
    "        sharpe_excess = comps_sharpe.subtract(market_sharpe, axis=0)\n",
    "        dummies_Return_beaters = sharpe_excess.copy()\n",
    "        dummies_Return_beaters[dummies_Return_beaters >= 0] =1\n",
    "        dummies_Return_beaters[dummies_Return_beaters < 0] =0\n",
    "        \n",
    "    return dummies_Return_beaters\n",
    "\n",
    "def port_simulation(stock_closed_df):\n",
    "    \n",
    "    pct_change_df = stock_closed_df.pct_change()\n",
    "    ind_er = pct_change_df.mean()\n",
    "    return_df = pct_change_df.apply(lambda x: np.log(1+x))\n",
    "    cov_matrix = return_df.cov()\n",
    "    \n",
    "    p_ret = [] \n",
    "    p_vol = [] \n",
    "    p_weights = [] \n",
    "    \n",
    "    \n",
    "    num_assets = len(stock_closed_df.columns)\n",
    "    num_portfolios = 10000\n",
    "\n",
    "    \n",
    "    for portfolio in range(num_portfolios):\n",
    "        weights = np.random.random(num_assets)\n",
    "        weights = weights/np.sum(weights)\n",
    "        p_weights.append(weights)\n",
    "        returns = np.dot(weights, ind_er)  \n",
    "        p_ret.append(returns)\n",
    "        var = cov_matrix.mul(weights, axis=0).mul(weights, axis=1).sum().sum()\n",
    "        sd = np.sqrt(var)  \n",
    "        p_vol.append(sd)\n",
    "    \n",
    "    data = {'Returns':p_ret, 'Volatility':p_vol}\n",
    " \n",
    "    for counter, symbol in enumerate(stock_closed_df.columns.tolist()):\n",
    "        data[symbol] = [w[counter] for w in p_weights]\n",
    "        portfolios  = pd.DataFrame(data)\n",
    "        \n",
    "    return portfolios\n",
    "\n",
    "def portfolios_return_df(equity_selection, all_return):\n",
    "    \n",
    "    simulation = port_simulation(equity_selection)\n",
    "    weight_possibility = simulation.drop(columns=[\"Returns\", \"Volatility\"])\n",
    "    portfolio_return_df = pd.DataFrame(columns = weight_possibility.index)\n",
    "    port_comps_return = all_return[weight_possibility.columns]\n",
    "    \n",
    "    for portfolio in tqdm(weight_possibility.index):\n",
    "        portfolio_return = weight_possibility.iloc[portfolio].mul(port_comps_return).T.sum()\n",
    "        portfolio_return_df[portfolio] = portfolio_return\n",
    "    \n",
    "    return portfolio_return_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FF_assign_label(index_comp_info):\n",
    "        \n",
    "        \n",
    "    index_comp_info[\"bookToMarket\"] = 1/index_comp_info[\"PB_ratio\"]\n",
    "    index_comp_info[\"Small_Big_Cap\"] = index_comp_info[\"mkt_cap\"].map(lambda x: \"B\" if x >= index_comp_info[\"mkt_cap\"].median() else \"S\")\n",
    "    \n",
    "    lower, upper = index_comp_info[\"bookToMarket\"].quantile([0.3, 0.7])\n",
    "    index_comp_info[\"HML_BP\"] = index_comp_info[\"bookToMarket\"].map(lambda x: \"H\" if x >= upper else \"M\")\n",
    "    index_comp_info[\"HML_BP\"] = index_comp_info.apply(lambda row: \"L\" if row[\"bookToMarket\"] <= lower else row[\"HML_BP\"], axis = 1)\n",
    "    \n",
    "    lower_roe, upper_roe = index_comp_info[\"ROE\"].quantile([0.3, 0.7])\n",
    "    index_comp_info[\"RNW_ROE\"] = index_comp_info[\"ROE\"].map(lambda x: \"R\" if x >= upper_roe else \"N\")\n",
    "    index_comp_info[\"RNW_ROE\"] = index_comp_info.apply(lambda row: \"W\" if row[\"ROE\"] <= lower_roe else row[\"RNW_ROE\"], axis = 1)\n",
    "    \n",
    "    lower_invest, upper_invest = index_comp_info[\"Asset_growth\"].quantile([0.3, 0.7])\n",
    "    index_comp_info[\"ANC_investment\"] = index_comp_info[\"Asset_growth\"].map(lambda x: \"A\" if x >= upper_invest else \"N\")\n",
    "    index_comp_info[\"ANC_investment\"] = index_comp_info.apply(lambda row: \"C\" if row[\"Asset_growth\"] <= lower_invest else row[\"ANC_investment\"], axis = 1)\n",
    "            \n",
    "    return index_comp_info\n",
    "        \n",
    "def FF_factor_classifier(index_comp_info_with_label):\n",
    "        \n",
    "    data = index_comp_info_with_label\n",
    "    Small_Low = data.query('(Small_Big_Cap==\"S\") & (HML_BP==\"L\")')\n",
    "    Small_Mid = data.query('(Small_Big_Cap==\"S\") & (HML_BP==\"M\")')\n",
    "    Small_High = data.query('(Small_Big_Cap==\"S\") & (HML_BP==\"H\")')\n",
    "    \n",
    "    Small_Weak = data.query('(Small_Big_Cap==\"S\") & (RNW_ROE==\"W\")')\n",
    "    Small_Neutral_Profit = data.query('(Small_Big_Cap==\"S\") & (RNW_ROE==\"N\")')\n",
    "    Small_Robust = data.query('(Small_Big_Cap==\"S\") & (RNW_ROE==\"R\")')\n",
    "    \n",
    "    Small_Conservative =  data.query('(Small_Big_Cap==\"S\") & (ANC_investment==\"C\")')\n",
    "    Small_Neutral_Invest =  data.query('(Small_Big_Cap==\"S\") & (ANC_investment==\"N\")')\n",
    "    Small_Aggresive =  data.query('(Small_Big_Cap==\"S\") & (ANC_investment==\"A\")')\n",
    "    \n",
    "    Big_Low = data.query('(Small_Big_Cap==\"B\") & (HML_BP==\"L\")')\n",
    "    Big_Mid = data.query('(Small_Big_Cap==\"B\") & (HML_BP==\"M\")')\n",
    "    Big_High = data.query('(Small_Big_Cap==\"B\") & (HML_BP==\"H\")')\n",
    "    \n",
    "    Big_Weak = data.query('(Small_Big_Cap==\"B\") & (RNW_ROE==\"W\")')\n",
    "    Big_Neutral_Profit = data.query('(Small_Big_Cap==\"B\") & (RNW_ROE==\"N\")')\n",
    "    Big_Robust = data.query('(Small_Big_Cap==\"B\") & (RNW_ROE==\"R\")')\n",
    "\n",
    "    Big_Conservative =  data.query('(Small_Big_Cap==\"B\") & (ANC_investment==\"C\")')\n",
    "    Big_Neutral_Invest = data.query('(Small_Big_Cap==\"B\") & (ANC_investment==\"N\")')\n",
    "    Big_Aggresive =  data.query('(Small_Big_Cap==\"B\") & (ANC_investment==\"A\")')\n",
    "    \n",
    "    each_groups_list = [Small_Low, Small_Mid, Small_High, \n",
    "                            Small_Weak, Small_Neutral_Profit, Small_Robust,\n",
    "                            Small_Conservative, Small_Neutral_Invest, Small_Aggresive,\n",
    "                            Big_Low, Big_Mid,Big_High,\n",
    "                            Big_Weak, Big_Neutral_Profit, Big_Robust,\n",
    "                            Big_Conservative, Big_Neutral_Invest, Big_Aggresive]\n",
    "        \n",
    "    return each_groups_list\n",
    "    \n",
    "def FF_classes_return(market_components_return, list_of_group_info, axis=True):\n",
    "        \n",
    "    groups_names = [\"Small_Low\", \"Small_Mid\", \"Small_High\",\n",
    "                        \"Small_Weak\", \"Small_Neutral_Profit\", \"Small_Robust\",\n",
    "                        \"Small_Cons\", \"Small_Neutral_Invest\", \"Small_Aggr\",\n",
    "                        \"Big_Low\", \"Big_Mid\",\"Big_High\",\n",
    "                        \"Big_Weak\", \"Big_Neutral_Profit\", \"Big_Robust\",\n",
    "                        \"Big_Cons\", \"Big_Neutral_Invest\", \"Big_Aggr\"]\n",
    "    \n",
    "    df_groups = pd.DataFrame(columns = groups_names)\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    for group in list_of_group_info:\n",
    "    \n",
    "        group_cap = group[\"mkt_cap\"].T\n",
    "        group_total_cap = group[\"mkt_cap\"].sum()\n",
    "        group_cap_multi_return = group_cap*market_components_return[list(group.index)]\n",
    "        \n",
    "        if axis == True:\n",
    "            df_groups[groups_names[counter]] = group_cap_multi_return.apply(lambda row: row.sum()/group_total_cap, axis=1)\n",
    "        \n",
    "        else:\n",
    "            groups_index_return = group_cap_multi_return.sum()/group_total_cap\n",
    "            df_groups[groups_names[counter]] = [groups_index_return]\n",
    "    \n",
    "        counter += 1\n",
    "                \n",
    "    return df_groups\n",
    "    \n",
    "def FF_calc_factors(classes_return_df, df = True):\n",
    "    \n",
    "    factor_name = [\"SMB\", \"HML\", \"RMW\", \"CMA\"]\n",
    "    \n",
    "    SMB_BP = (classes_return_df[\"Small_Low\"] + classes_return_df[\"Small_Mid\"] \n",
    "                      + classes_return_df[\"Small_High\"]) - (classes_return_df[\"Big_Low\"]\n",
    "                      + classes_return_df[\"Big_Mid\"] + classes_return_df[\"Big_High\"])/3\n",
    "    \n",
    "    SMB_PFT = (classes_return_df[\"Small_Weak\"] + classes_return_df[\"Small_Neutral_Profit\"] \n",
    "                      + classes_return_df[\"Small_Robust\"]) - (classes_return_df[\"Big_Weak\"]\n",
    "                      + classes_return_df[\"Big_Neutral_Profit\"] + classes_return_df[\"Big_Robust\"])/3\n",
    "    \n",
    "    SMB_INV = (classes_return_df[\"Small_Cons\"] + classes_return_df[\"Small_Neutral_Invest\"] \n",
    "                      + classes_return_df[\"Small_Aggr\"]) - (classes_return_df[\"Big_Cons\"]\n",
    "                      + classes_return_df[\"Big_Neutral_Invest\"] + classes_return_df[\"Big_Aggr\"])/3\n",
    "    \n",
    "    if df == True:\n",
    "        \n",
    "        FF_factors_data = pd.DataFrame(columns = factor_name)\n",
    "        \n",
    "    \n",
    "    FF_factors_data[\"SMB\"] = (SMB_BP + SMB_PFT + SMB_INV)/3\n",
    "    \n",
    "    FF_factors_data[\"HML\"] = (classes_return_df[\"Small_High\"] + classes_return_df[\"Big_High\"]\n",
    "                      - (classes_return_df[\"Small_Low\"] + classes_return_df[\"Big_Low\"])) / 2\n",
    "    \n",
    "    FF_factors_data[\"RMW\"] = (classes_return_df[\"Small_Robust\"] + classes_return_df[\"Big_Robust\"]\n",
    "                      - (classes_return_df[\"Small_Weak\"] + classes_return_df[\"Big_Weak\"])) / 2\n",
    "    \n",
    "    FF_factors_data[\"CMA\"] = (classes_return_df[\"Small_Cons\"] + classes_return_df[\"Big_Cons\"]\n",
    "                      - (classes_return_df[\"Small_Aggr\"] + classes_return_df[\"Big_Aggr\"])) / 2\n",
    "        \n",
    "    return FF_factors_data\n",
    "\n",
    "    \n",
    "def FF_regress(FF_factors_df, target_comp_risk_premium, modelType = 'linear', transform = False):\n",
    "           \n",
    "    y = target_comp_risk_premium\n",
    "    poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "    poly_X = poly.fit_transform(FF_factors_df)\n",
    "    poly_FF = pd.DataFrame(data = poly_X,index=FF_factors_df.index)\n",
    "        \n",
    "    if modelType == 'lasso':\n",
    "        \n",
    "        X = poly_FF\n",
    "        \n",
    "        model = linear_model.Lasso(alpha = 0.00000001)\n",
    "        model.fit(X.astype(float), y.astype(float))\n",
    "        params = np.append(model.intercept_,model.coef_)\n",
    "        predictions = model.predict(X)\n",
    "        \n",
    "    elif modelType == 'linear':\n",
    "        \n",
    "        if transform == True:\n",
    "            X = poly_FF\n",
    "        \n",
    "        elif transform == False:\n",
    "            X = FF_factors_df\n",
    "            \n",
    "        model = linear_model.LinearRegression()\n",
    "        model.fit(X.astype(float), y.astype(float))\n",
    "        params = np.append(model.intercept_,model.coef_)\n",
    "        predictions = model.predict(X)\n",
    "        \n",
    "    elif modelType == 'ridge':\n",
    "    \n",
    "        if transform == True:\n",
    "            X = poly_FF\n",
    "            set_alphas=[0.000001,0.0000001]\n",
    "            \n",
    "        elif transform == False:\n",
    "            X = FF_factors_df\n",
    "            set_alphas = [0.00000001,0.0000001]\n",
    "            \n",
    "        model = linear_model.RidgeCV(alphas = set_alphas, cv=5)\n",
    "        model.fit(X.astype(float), y.astype(float))\n",
    "        params = np.append(model.intercept_,model.coef_)\n",
    "        predictions = model.predict(X)\n",
    "        \n",
    "    elif modelType == 'elastic':\n",
    "        \n",
    "        if transform == True:\n",
    "            X = poly_FF\n",
    "            set_alphas = [0.000001,0.0000001]\n",
    "        \n",
    "        elif transform == False:\n",
    "            X = FF_factors_df\n",
    "            set_alphas = [0.00000001,0.0000001]\n",
    "            \n",
    "        model = linear_model.ElasticNetCV(alphas = set_alphas, cv=5, random_state=0)\n",
    "        model.fit(X.astype(float), y.astype(float))\n",
    "        params = np.append(model.intercept_,model.coef_)\n",
    "        predictions = model.predict(X)\n",
    "\n",
    "    newX = pd.DataFrame({\"Constant\":np.ones(len(X))}).join(pd.DataFrame(X.reset_index(drop=True)))\n",
    "    MSE = (sum((y-predictions)**2))/(len(newX)-len(newX.columns))\n",
    "\n",
    "    var_b = MSE*(np.linalg.inv(np.dot(newX.T,newX)).diagonal())\n",
    "    sd_b = np.sqrt(var_b)\n",
    "    ts_b = params/ sd_b\n",
    "    \n",
    "    p_values =[2*(1-stats.t.cdf(np.abs(i),(len(newX)-len(X.columns)))) for i in ts_b]\n",
    "\n",
    "    sd_b = np.round(sd_b,3)\n",
    "    ts_b = np.round(ts_b,3)\n",
    "    p_values = np.round(p_values,3)\n",
    "    params = np.round(params,4)\n",
    "\n",
    "    myDF3 = pd.DataFrame()\n",
    "    myDF3[\"Coefficients\"],myDF3[\"Standard_Errors\"],myDF3[\"t_stat\"],myDF3[\"P_value\"] = [params,sd_b,ts_b,p_values]\n",
    "    R2 = r2_score(y,predictions)\n",
    "\n",
    "    return myDF3 , R2\n",
    "\n",
    "def regressAll(FF_factors, all_returns, days, train_base=2000, Rf=0.000001, modelType = 'linear', transform=False):\n",
    "    \n",
    "\n",
    "    dummies_Return_beaters = dummies_beater(all_returns, Rf)\n",
    "\n",
    "    prob_10obs = dummies_Return_beaters.iloc[days - 10 : days].mean().T\n",
    "    prob_20obs = dummies_Return_beaters.iloc[days - 20 : days].mean().T\n",
    "    prob_30obs = dummies_Return_beaters.iloc[days - 30 : days].mean().T\n",
    "\n",
    "    target = dummies_Return_beaters.iloc[days+1]\n",
    "    \n",
    "    if modelType == \"lasso\":\n",
    "        transform = True\n",
    "\n",
    "    if transform == True:\n",
    "        features_df = pd.DataFrame(columns = [\"const\",'B1', 'B2', 'B3', 'B4', 'B5'\n",
    "                                              ,'B6', 'B7', 'B8', 'B9', 'B10',\"B11\"\n",
    "                                              ,\"B12\",\"B13\",\"B14\",\"B15\"\n",
    "                                              ,\"Pval_C\", \"Pval1\", \"Pval2\", \"Pval3\", \"Pval4\", \"Pval5\"\n",
    "                                              ,\"Pval6\", \"Pval7\", \"Pval8\", \"Pval9\", \"Pval10\", \"Pval11\"\n",
    "                                              ,\"Pval12\",\"Pval13\", \"Pval14\",\"Pval15\"\n",
    "                                              , 'R2'],\n",
    "                               index = all_returns.columns)\n",
    "        \n",
    "    else:    \n",
    "        features_df = pd.DataFrame(columns = [\"const\",'B1', 'B2', 'B3', 'B4', 'B5',\"Pval_C\", \"Pval1\", \"Pval2\", \"Pval3\", \"Pval4\", \"Pval5\", 'R2'],\n",
    "                               index = all_returns.columns)\n",
    "\n",
    "    comps_R = all_returns.drop(columns = [\"SPY\"]).fillna(0)\n",
    "    FF_factors = FF_factors.fillna(0)\n",
    "    for i in comps_R:\n",
    "\n",
    "        regression_stat, R2 = FF_regress(FF_factors.iloc[days-train_base:days], comps_R[i].iloc[days-train_base:days], modelType, transform)\n",
    "        betas = list(regression_stat[\"Coefficients\"])\n",
    "        Pvals = list(regression_stat[\"P_value\"])\n",
    "        features = betas + Pvals\n",
    "        features.append(R2)\n",
    "        features_df.loc[i] = features\n",
    "\n",
    "    features_df[\"prob_10obs\"], features_df[\"prob_20obs\"], features_df[\"prob_30obs\"] = [prob_10obs, prob_20obs, prob_30obs]\n",
    "    features_df[\"true_target\"] = target\n",
    "\n",
    "    return features_df\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(os.getcwd())\n",
    "#print(os.listdir(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "FF_info = pd.read_csv(\"Fama_French_info.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use below link to download 2min data we use in the project\n",
    "link = \"https://drive.google.com/file/d/1-sHk__G9d3rz5FY7-T6B5ZqRJdmzoKvH/view?usp=sharing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for downloading sp500 stock price data\n",
    "str1 = ' '\n",
    "index_list = FF_info.index.tolist()\n",
    "index_list.append(\"SPY\")\n",
    "#index_list.reverse()\n",
    "total_string = str1.join(index_list)\n",
    "#SPYn500_2m_df = yf.download(total_string, start = , end = , interval = '2m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_2m_close = pd.read_csv(\"SPYn500_2m_close.csv\", index_col = 0)\n",
    "#sp_30m_close = pd.read_csv(\"data/SPYn500_30m_close.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>AAL</th>\n",
       "      <th>AAP</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>ABBV</th>\n",
       "      <th>ABC</th>\n",
       "      <th>ABMD</th>\n",
       "      <th>ABT</th>\n",
       "      <th>ACN</th>\n",
       "      <th>ADBE</th>\n",
       "      <th>...</th>\n",
       "      <th>XEL</th>\n",
       "      <th>XLNX</th>\n",
       "      <th>XOM</th>\n",
       "      <th>XRAY</th>\n",
       "      <th>XYL</th>\n",
       "      <th>YUM</th>\n",
       "      <th>ZBH</th>\n",
       "      <th>ZBRA</th>\n",
       "      <th>ZION</th>\n",
       "      <th>ZTS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-10-11 09:30:00-04:00</th>\n",
       "      <td>150.729996</td>\n",
       "      <td>20.120001</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>141.949997</td>\n",
       "      <td>111.199997</td>\n",
       "      <td>120.519997</td>\n",
       "      <td>333.580200</td>\n",
       "      <td>118.260002</td>\n",
       "      <td>324.829987</td>\n",
       "      <td>571.551025</td>\n",
       "      <td>...</td>\n",
       "      <td>61.740002</td>\n",
       "      <td>154.020004</td>\n",
       "      <td>62.622299</td>\n",
       "      <td>58.139999</td>\n",
       "      <td>121.169998</td>\n",
       "      <td>123.730003</td>\n",
       "      <td>146.309998</td>\n",
       "      <td>492.0</td>\n",
       "      <td>64.209999</td>\n",
       "      <td>196.289993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-11 09:32:00-04:00</th>\n",
       "      <td>150.690002</td>\n",
       "      <td>20.020000</td>\n",
       "      <td>213.889999</td>\n",
       "      <td>142.850006</td>\n",
       "      <td>111.160004</td>\n",
       "      <td>121.260002</td>\n",
       "      <td>333.104889</td>\n",
       "      <td>118.154999</td>\n",
       "      <td>325.119995</td>\n",
       "      <td>574.900024</td>\n",
       "      <td>...</td>\n",
       "      <td>62.064999</td>\n",
       "      <td>155.339996</td>\n",
       "      <td>62.560001</td>\n",
       "      <td>58.119999</td>\n",
       "      <td>121.239998</td>\n",
       "      <td>123.787201</td>\n",
       "      <td>146.315002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>63.900002</td>\n",
       "      <td>196.509995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-11 09:34:00-04:00</th>\n",
       "      <td>151.330002</td>\n",
       "      <td>19.870001</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>142.889999</td>\n",
       "      <td>111.129997</td>\n",
       "      <td>121.184998</td>\n",
       "      <td>335.989990</td>\n",
       "      <td>118.599998</td>\n",
       "      <td>324.910004</td>\n",
       "      <td>575.900024</td>\n",
       "      <td>...</td>\n",
       "      <td>62.360001</td>\n",
       "      <td>155.895004</td>\n",
       "      <td>62.556599</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>120.980003</td>\n",
       "      <td>123.180000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>63.650002</td>\n",
       "      <td>196.820007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 506 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    A        AAL         AAP        AAPL  \\\n",
       "Datetime                                                                   \n",
       "2021-10-11 09:30:00-04:00  150.729996  20.120001  214.000000  141.949997   \n",
       "2021-10-11 09:32:00-04:00  150.690002  20.020000  213.889999  142.850006   \n",
       "2021-10-11 09:34:00-04:00  151.330002  19.870001  214.000000  142.889999   \n",
       "\n",
       "                                 ABBV         ABC        ABMD         ABT  \\\n",
       "Datetime                                                                    \n",
       "2021-10-11 09:30:00-04:00  111.199997  120.519997  333.580200  118.260002   \n",
       "2021-10-11 09:32:00-04:00  111.160004  121.260002  333.104889  118.154999   \n",
       "2021-10-11 09:34:00-04:00  111.129997  121.184998  335.989990  118.599998   \n",
       "\n",
       "                                  ACN        ADBE  ...        XEL        XLNX  \\\n",
       "Datetime                                           ...                          \n",
       "2021-10-11 09:30:00-04:00  324.829987  571.551025  ...  61.740002  154.020004   \n",
       "2021-10-11 09:32:00-04:00  325.119995  574.900024  ...  62.064999  155.339996   \n",
       "2021-10-11 09:34:00-04:00  324.910004  575.900024  ...  62.360001  155.895004   \n",
       "\n",
       "                                 XOM       XRAY         XYL         YUM  \\\n",
       "Datetime                                                                  \n",
       "2021-10-11 09:30:00-04:00  62.622299  58.139999  121.169998  123.730003   \n",
       "2021-10-11 09:32:00-04:00  62.560001  58.119999  121.239998  123.787201   \n",
       "2021-10-11 09:34:00-04:00  62.556599  58.000000  120.980003  123.180000   \n",
       "\n",
       "                                  ZBH   ZBRA       ZION         ZTS  \n",
       "Datetime                                                             \n",
       "2021-10-11 09:30:00-04:00  146.309998  492.0  64.209999  196.289993  \n",
       "2021-10-11 09:32:00-04:00  146.315002    NaN  63.900002  196.509995  \n",
       "2021-10-11 09:34:00-04:00         NaN    NaN  63.650002  196.820007  \n",
       "\n",
       "[3 rows x 506 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_2m_close.iloc[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate log-return (assuming return is log-normal) for both time interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_2m_return = return_df(sp_2m_close)\n",
    "#total_30m_return = return_df(sp_30m_close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First row is always NaN\n",
    "total_2m_return = total_2m_return.dropna(how = \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill NaN with 0 in return df because NaN value represent a number too close to 0\n",
    "total_2m_return = total_2m_return.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>AAL</th>\n",
       "      <th>AAP</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>ABBV</th>\n",
       "      <th>ABC</th>\n",
       "      <th>ABMD</th>\n",
       "      <th>ABT</th>\n",
       "      <th>ACN</th>\n",
       "      <th>ADBE</th>\n",
       "      <th>...</th>\n",
       "      <th>XEL</th>\n",
       "      <th>XLNX</th>\n",
       "      <th>XOM</th>\n",
       "      <th>XRAY</th>\n",
       "      <th>XYL</th>\n",
       "      <th>YUM</th>\n",
       "      <th>ZBH</th>\n",
       "      <th>ZBRA</th>\n",
       "      <th>ZION</th>\n",
       "      <th>ZTS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-10-11 09:32:00-04:00</th>\n",
       "      <td>-0.000265</td>\n",
       "      <td>-0.004983</td>\n",
       "      <td>-0.000514</td>\n",
       "      <td>0.006320</td>\n",
       "      <td>-0.000360</td>\n",
       "      <td>0.006121</td>\n",
       "      <td>-0.001426</td>\n",
       "      <td>-0.000888</td>\n",
       "      <td>0.000892</td>\n",
       "      <td>0.005842</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005250</td>\n",
       "      <td>0.008534</td>\n",
       "      <td>-0.000995</td>\n",
       "      <td>-0.000344</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.00484</td>\n",
       "      <td>0.001120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-11 09:34:00-04:00</th>\n",
       "      <td>0.004238</td>\n",
       "      <td>-0.007521</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>-0.000270</td>\n",
       "      <td>-0.000619</td>\n",
       "      <td>0.008624</td>\n",
       "      <td>0.003759</td>\n",
       "      <td>-0.000646</td>\n",
       "      <td>0.001738</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004742</td>\n",
       "      <td>0.003566</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.002067</td>\n",
       "      <td>-0.002147</td>\n",
       "      <td>-0.004917</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.00392</td>\n",
       "      <td>0.001576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-11 09:36:00-04:00</th>\n",
       "      <td>-0.004039</td>\n",
       "      <td>0.002789</td>\n",
       "      <td>-0.000187</td>\n",
       "      <td>-0.001891</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.003336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.003886</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>-0.001808</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000802</td>\n",
       "      <td>0.000673</td>\n",
       "      <td>0.003406</td>\n",
       "      <td>-0.001035</td>\n",
       "      <td>0.001280</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.001264</td>\n",
       "      <td>0.002923</td>\n",
       "      <td>0.00157</td>\n",
       "      <td>-0.000762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 506 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  A       AAL       AAP      AAPL      ABBV  \\\n",
       "Datetime                                                                      \n",
       "2021-10-11 09:32:00-04:00 -0.000265 -0.004983 -0.000514  0.006320 -0.000360   \n",
       "2021-10-11 09:34:00-04:00  0.004238 -0.007521  0.000514  0.000280 -0.000270   \n",
       "2021-10-11 09:36:00-04:00 -0.004039  0.002789 -0.000187 -0.001891  0.000315   \n",
       "\n",
       "                                ABC      ABMD       ABT       ACN      ADBE  \\\n",
       "Datetime                                                                      \n",
       "2021-10-11 09:32:00-04:00  0.006121 -0.001426 -0.000888  0.000892  0.005842   \n",
       "2021-10-11 09:34:00-04:00 -0.000619  0.008624  0.003759 -0.000646  0.001738   \n",
       "2021-10-11 09:36:00-04:00  0.003336  0.000000 -0.003886  0.000062 -0.001808   \n",
       "\n",
       "                           ...       XEL      XLNX       XOM      XRAY  \\\n",
       "Datetime                   ...                                           \n",
       "2021-10-11 09:32:00-04:00  ...  0.005250  0.008534 -0.000995 -0.000344   \n",
       "2021-10-11 09:34:00-04:00  ...  0.004742  0.003566 -0.000054 -0.002067   \n",
       "2021-10-11 09:36:00-04:00  ... -0.000802  0.000673  0.003406 -0.001035   \n",
       "\n",
       "                                XYL       YUM       ZBH      ZBRA     ZION  \\\n",
       "Datetime                                                                     \n",
       "2021-10-11 09:32:00-04:00  0.000578  0.000462  0.000034  0.000000 -0.00484   \n",
       "2021-10-11 09:34:00-04:00 -0.002147 -0.004917  0.000000  0.000000 -0.00392   \n",
       "2021-10-11 09:36:00-04:00  0.001280  0.000244  0.001264  0.002923  0.00157   \n",
       "\n",
       "                                ZTS  \n",
       "Datetime                             \n",
       "2021-10-11 09:32:00-04:00  0.001120  \n",
       "2021-10-11 09:34:00-04:00  0.001576  \n",
       "2021-10-11 09:36:00-04:00 -0.000762  \n",
       "\n",
       "[3 rows x 506 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_2m_return.iloc[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining components return, market return, dummies beater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_2m_return = total_2m_return.drop(columns=[\"SPY\"]) #seperate market ETF from other companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rm_2m = total_2m_return[['SPY']]\n",
    "#need to discount Monthly Rf to 2m Rf using yield curve(continuously compounded) when doing real time analysis\n",
    "Rf = 0.000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies_2m_beater = dummies_beater(total_2m_return, Rf, option=\"sharpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>AAL</th>\n",
       "      <th>AAP</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>ABBV</th>\n",
       "      <th>ABC</th>\n",
       "      <th>ABMD</th>\n",
       "      <th>ABT</th>\n",
       "      <th>ACN</th>\n",
       "      <th>ADBE</th>\n",
       "      <th>...</th>\n",
       "      <th>XEL</th>\n",
       "      <th>XLNX</th>\n",
       "      <th>XOM</th>\n",
       "      <th>XRAY</th>\n",
       "      <th>XYL</th>\n",
       "      <th>YUM</th>\n",
       "      <th>ZBH</th>\n",
       "      <th>ZBRA</th>\n",
       "      <th>ZION</th>\n",
       "      <th>ZTS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-10-11 09:32:00-04:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-11 09:34:00-04:00</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-11 09:36:00-04:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 505 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             A  AAL  AAP  AAPL  ABBV  ABC  ABMD  ABT  ACN  \\\n",
       "Datetime                                                                    \n",
       "2021-10-11 09:32:00-04:00  0.0  0.0  0.0   1.0   0.0  1.0   0.0  0.0  0.0   \n",
       "2021-10-11 09:34:00-04:00  1.0  0.0  1.0   1.0   1.0  1.0   1.0  1.0  1.0   \n",
       "2021-10-11 09:36:00-04:00  0.0  1.0  0.0   0.0   0.0  1.0   0.0  0.0  0.0   \n",
       "\n",
       "                           ADBE  ...  XEL  XLNX  XOM  XRAY  XYL  YUM  ZBH  \\\n",
       "Datetime                         ...                                        \n",
       "2021-10-11 09:32:00-04:00   1.0  ...  1.0   1.0  0.0   0.0  0.0  0.0  0.0   \n",
       "2021-10-11 09:34:00-04:00   1.0  ...  1.0   1.0  1.0   1.0  1.0  0.0  1.0   \n",
       "2021-10-11 09:36:00-04:00   0.0  ...  0.0   1.0  1.0   0.0  1.0  0.0  1.0   \n",
       "\n",
       "                           ZBRA  ZION  ZTS  \n",
       "Datetime                                    \n",
       "2021-10-11 09:32:00-04:00   0.0   0.0  0.0  \n",
       "2021-10-11 09:34:00-04:00   1.0   0.0  1.0  \n",
       "2021-10-11 09:36:00-04:00   1.0   1.0  0.0  \n",
       "\n",
       "[3 rows x 505 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummies_2m_beater.iloc[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate FF factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "FF_info_labeled = FF_assign_label(FF_info)\n",
    "FF_classified_list = FF_factor_classifier(FF_info_labeled)\n",
    "FF_2m_classes_return = FF_classes_return(sp500_2m_return,FF_classified_list)\n",
    "FF_2m_factor = FF_calc_factors(FF_2m_classes_return)\n",
    "FF_2m_factor[\"Rm-Rf\"] = Rm_2m - Rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill NaN with 0 for FF factors because the value is too small\n",
    "FF_2m_factor = FF_2m_factor.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SMB</th>\n",
       "      <th>HML</th>\n",
       "      <th>RMW</th>\n",
       "      <th>CMA</th>\n",
       "      <th>Rm-Rf</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-10-11 09:32:00-04:00</th>\n",
       "      <td>-0.001462</td>\n",
       "      <td>-0.003179</td>\n",
       "      <td>0.001835</td>\n",
       "      <td>-0.000872</td>\n",
       "      <td>0.001503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-11 09:34:00-04:00</th>\n",
       "      <td>-0.005193</td>\n",
       "      <td>-0.002111</td>\n",
       "      <td>0.001227</td>\n",
       "      <td>-0.000469</td>\n",
       "      <td>-0.001025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-11 09:36:00-04:00</th>\n",
       "      <td>0.004055</td>\n",
       "      <td>0.001614</td>\n",
       "      <td>-0.001921</td>\n",
       "      <td>0.001213</td>\n",
       "      <td>0.000136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                SMB       HML       RMW       CMA     Rm-Rf\n",
       "Datetime                                                                   \n",
       "2021-10-11 09:32:00-04:00 -0.001462 -0.003179  0.001835 -0.000872  0.001503\n",
       "2021-10-11 09:34:00-04:00 -0.005193 -0.002111  0.001227 -0.000469 -0.001025\n",
       "2021-10-11 09:36:00-04:00  0.004055  0.001614 -0.001921  0.001213  0.000136"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FF_2m_factor.iloc[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate features and relevant statistic for given company "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of multi-collinearity between FF factors, we need to incorporate models like ridge regression and elastic net into parameters estimation process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   Coefficients  Standard_Errors  t_stat  P_value\n",
       " 0        0.0000            0.000   0.325    0.745\n",
       " 1       -0.3181            0.012 -27.625    0.000\n",
       " 2       -0.4702            0.022 -21.836    0.000\n",
       " 3        0.6130            0.024  25.983    0.000\n",
       " 4        1.4745            0.030  49.395    0.000\n",
       " 5        1.6016            0.027  59.135    0.000,\n",
       " 0.5610279415110841)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Default argument is linear model without polynimio features \n",
    "FF_regress(FF_2m_factor, sp500_2m_return[\"AAPL\"]-Rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(    Coefficients  Standard_Errors  t_stat  P_value\n",
       " 0         0.0000            0.000   0.391    0.696\n",
       " 1        -0.3057            0.012 -25.817    0.000\n",
       " 2        -0.4700            0.023 -20.178    0.000\n",
       " 3         0.5877            0.027  21.563    0.000\n",
       " 4         1.3688            0.034  40.837    0.000\n",
       " 5         1.6059            0.027  58.914    0.000\n",
       " 6        10.0791           10.604   0.950    0.342\n",
       " 7         0.7609           13.778   0.055    0.956\n",
       " 8        11.1005           16.400   0.677    0.499\n",
       " 9        -8.8915            4.051  -2.195    0.028\n",
       " 10        0.9623           20.723   0.046    0.963\n",
       " 11       -5.5943           11.254  -0.497    0.619\n",
       " 12        6.8146           26.527   0.257    0.797\n",
       " 13        1.1029           16.183   0.068    0.946\n",
       " 14       -1.0358           44.089  -0.023    0.981\n",
       " 15        6.0549           40.793   0.148    0.882,\n",
       " 0.5948224015386481)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#other regression models can be used by specifing in argument; modelType is for model selection and transform is for polynomio feature transformation\n",
    "FF_regress(FF_2m_factor, sp500_2m_return[\"AAPL\"]-Rf, modelType=\"ridge\", transform = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe926350d172495da11a2bd7971fcdda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f497951f670>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "time_length = range(len(dummies_2m_beater.index[:1000]))\n",
    "winner_rate_list = []\n",
    "for length in tqdm(time_length):\n",
    "    winner_rate = dummies_2m_beater[\"AAPL\"].iloc[:length].mean()\n",
    "    winner_rate_list.append(winner_rate)\n",
    "    \n",
    "plt.scatter(time_length, winner_rate_list, s=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to predict wheather a given security or portfolio will outperform the market in the next trading period, so we estimate factor exposure based on a given amount of most recent observations at T0 and use the exposures+relevant statistic to predict the outcome at T1. Since the probability of beating the market will converge to 0.5 as observation increase, we also include realized probabilities that are observe from last 10, 20, and 30 observations as features. We will roll the model and make prediction for at least 10 trading period, and will collect accuracy metrics of time-series prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performence_df(FF_2m_factor,total_2m_return, Rf, modelType = \"linear\", transform = False):\n",
    "\n",
    "    performence = pd.DataFrame(columns=[\"precision\", \"recall\", \"f1\"])\n",
    "\n",
    "    days = 150\n",
    "\n",
    "    while days < 152:\n",
    "        #split\n",
    "        data = regressAll(FF_2m_factor,total_2m_return,days,train_base = 150,Rf = Rf, modelType = modelType, transform = transform)\n",
    "        data = data.fillna(0)\n",
    "        split = int(len(data)*0.6)\n",
    "        training_set = data.iloc[:split]\n",
    "        testing_set = data.iloc[split:]\n",
    "        \n",
    "        sgd_clf = SGDClassifier(random_state = 42,loss = 'modified_huber')\n",
    "        model1 = sgd_clf.fit(training_set.drop(columns=[\"true_target\"]),training_set[\"true_target\"])\n",
    "        log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
    "        rnd_clf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "        svm_clf = SVC(gamma=\"scale\", random_state=42, probability=True)\n",
    "        model2 = log_clf.fit(training_set.drop(columns=[\"true_target\"]),training_set[\"true_target\"])\n",
    "        model3 = rnd_clf.fit(training_set.drop(columns=[\"true_target\"]),training_set[\"true_target\"])\n",
    "        model4 = svm_clf.fit(training_set.drop(columns=[\"true_target\"]),training_set[\"true_target\"])\n",
    "        voting_clf = VotingClassifier(\n",
    "        estimators=[('sgd',sgd_clf),('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "        voting='soft')\n",
    "        final_model = voting_clf.fit(training_set.drop(columns=[\"true_target\"]),training_set[\"true_target\"])\n",
    "    \n",
    "        prediction = final_model.predict(testing_set.drop(columns=[\"true_target\"]))\n",
    "        testing_set = testing_set.copy()\n",
    "        testing_set[\"prediction\"] = prediction\n",
    "        \n",
    "        precision = precision_score(testing_set[\"true_target\"], testing_set[\"prediction\"])\n",
    "        recall = recall_score(testing_set[\"true_target\"], testing_set[\"prediction\"])\n",
    "        f1 = f1_score(testing_set[\"true_target\"], testing_set[\"prediction\"])\n",
    "    \n",
    "        score_series = pd.Series([precision, recall, f1], index = performence.columns)\n",
    "        performence = performence.append(score_series, ignore_index=True)\n",
    "    \n",
    "        days += 1\n",
    "    \n",
    "    return performence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performence_df(FF_2m_factor,total_2m_return, Rf, modelType = \"linear\", transform = False):\n",
    "\n",
    "    performence = pd.DataFrame(columns=[\"precision\", \"recall\", \"f1\"])\n",
    "\n",
    "    days = 150\n",
    "\n",
    "    while days < 160:\n",
    "        #split\n",
    "        data = regressAll(FF_2m_factor,total_2m_return,days,train_base = 150,Rf = Rf, modelType = modelType, transform = transform)\n",
    "        data = data.fillna(0)\n",
    "        split = int(len(data)*0.6)\n",
    "        training_set = data.iloc[:split]\n",
    "        testing_set = data.iloc[split:]\n",
    "        \n",
    "        sgd_clf = SGDClassifier(random_state = 42,loss = 'modified_huber')\n",
    "        model1 = sgd_clf.fit(training_set.drop(columns=[\"true_target\"]),training_set[\"true_target\"])\n",
    "        log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
    "        rnd_clf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "        svm_clf = SVC(gamma=\"scale\", random_state=42, probability=True)\n",
    "        model1 = sgd_clf.fit(training_set.drop(columns=[\"true_target\"]),training_set[\"true_target\"])\n",
    "        model2 = log_clf.fit(training_set.drop(columns=[\"true_target\"]),training_set[\"true_target\"])\n",
    "        model3 = rnd_clf.fit(training_set.drop(columns=[\"true_target\"]),training_set[\"true_target\"])\n",
    "        model4 = svm_clf.fit(training_set.drop(columns=[\"true_target\"]),training_set[\"true_target\"])\n",
    "        \n",
    "        bag_clf1 = BaggingClassifier(SGDClassifier(), n_estimators=500, max_samples=100, bootstrap=True, n_jobs=-1)\n",
    "        model5 = bag_clf1.fit(training_set.drop(columns=[\"true_target\"]),training_set[\"true_target\"])\n",
    "        y_pred1 = bag_clf1.predict(testing_set.drop(columns=[\"true_target\"]))\n",
    "        \n",
    "        bag_clf2 = BaggingClassifier(LogisticRegression(), n_estimators=500, max_samples=100, bootstrap=True, n_jobs=-1)\n",
    "        model6 = bag_clf2.fit(training_set.drop(columns=[\"true_target\"]),training_set[\"true_target\"])\n",
    "        y_pred2 = bag_clf1.predict(testing_set.drop(columns=[\"true_target\"]))\n",
    "        \n",
    "        bag_clf3 = BaggingClassifier(RandomForestClassifier(), n_estimators=500, max_samples=100, bootstrap=True, n_jobs=-1)\n",
    "        model7 = bag_clf3.fit(training_set.drop(columns=[\"true_target\"]),training_set[\"true_target\"])\n",
    "        y_pred3 = bag_clf3.predict(testing_set.drop(columns=[\"true_target\"]))\n",
    "        \n",
    "        bag_clf4 = BaggingClassifier(SVC(), n_estimators=500, max_samples=100, bootstrap=True, n_jobs=-1)\n",
    "        model8 = bag_clf4.fit(training_set.drop(columns=[\"true_target\"]),training_set[\"true_target\"])\n",
    "        y_pred4 = bag_clf4.predict(testing_set.drop(columns=[\"true_target\"]))\n",
    "        \n",
    "        \n",
    "        voting_clf1 = VotingClassifier(\n",
    "        estimators=[('sgd',sgd_clf),('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "        voting='soft')\n",
    "        final_model = voting_clf1.fit(training_set.drop(columns=[\"true_target\"]),training_set[\"true_target\"])\n",
    "        \n",
    "        voting_clf2 = VotingClassifier(\n",
    "        estimators=[('bag1', model5),('bag2', model6), ('bag3', model7), ('bag4', model8)],\n",
    "        voting='soft')\n",
    "        final = voting_clf2.fit(training_set.drop(columns=[\"true_target\"]),training_set[\"true_target\"])\n",
    "        \n",
    "        voting_clf_final = VotingClassifier(\n",
    "        estimators=[('first', voting_clf1),('second', voting_clf2)],\n",
    "        voting='soft')\n",
    "        real_final_model = voting_clf_final.fit(training_set.drop(columns=[\"true_target\"]),training_set[\"true_target\"])\n",
    "    \n",
    "        prediction = final.predict(testing_set.drop(columns=[\"true_target\"]))\n",
    "        testing_set = testing_set.copy()\n",
    "        testing_set[\"prediction\"] = prediction\n",
    "        \n",
    "        precision = precision_score(testing_set[\"true_target\"], testing_set[\"prediction\"])\n",
    "        recall = recall_score(testing_set[\"true_target\"], testing_set[\"prediction\"])\n",
    "        f1 = f1_score(testing_set[\"true_target\"], testing_set[\"prediction\"])\n",
    "    \n",
    "        score_series = pd.Series([precision, recall, f1], index = performence.columns)\n",
    "        performence = performence.append(score_series, ignore_index=True)\n",
    "    \n",
    "        days += 1\n",
    "    \n",
    "    return performence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = performence_df(FF_2m_factor,total_2m_return, Rf=Rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "precision    0.710744\n",
       "recall       0.716667\n",
       "f1           0.713693\n",
       "dtype: float64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = performence_df(FF_2m_factor,total_2m_return, Rf=Rf, modelType=\"lasso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3 = performence_df(FF_2m_factor,total_2m_return, Rf=Rf, modelType=\"elastic\", transform=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p4 = performence_df(FF_2m_factor,total_2m_return, Rf=Rf, modelType=\"ridge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p4.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p5 =  performence_df(FF_2m_factor,total_2m_return, Rf=Rf, modelType=\"ridge\", transform=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p5.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p6 = performence_df(FF_2m_factor,total_2m_return, Rf=Rf, modelType=\"linear\", transform=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p6.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
